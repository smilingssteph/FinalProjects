{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "8f3439e7-a5f4-4c0c-b2f9-d6aa457ae2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import osmnx as ox\n",
    "import networkx as nx\n",
    "from scipy.sparse import csr_matrix\n",
    "import scipy.sparse as sp\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "from collections import namedtuple\n",
    "\n",
    "import dgl\n",
    "from dgl.nn import GraphConv\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ed0117-a4dc-46d3-b64a-e8acb8b53da3",
   "metadata": {},
   "source": [
    "### Data Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cdf84236-fe7e-45ff-b580-2922e540861c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_pickle(\"data/training/Graph_State_Uni.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "444665b5-ecb6-4df7-8394-892ec1db96ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Experiment</th>\n",
       "      <th>Town</th>\n",
       "      <th>County</th>\n",
       "      <th>State</th>\n",
       "      <th>Treatment</th>\n",
       "      <th>Search_Place</th>\n",
       "      <th>Graph</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(AlAMU, 1875)</td>\n",
       "      <td>Huntsville</td>\n",
       "      <td>Madison</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>1</td>\n",
       "      <td>Huntsville, Alabama</td>\n",
       "      <td>(56922621, 56923776, 56923781, 56923787, 56924...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(AlAMU, 1875)</td>\n",
       "      <td>Montgomery</td>\n",
       "      <td>Montgomery</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>0</td>\n",
       "      <td>Montgomery, Alabama</td>\n",
       "      <td>(58878326, 58878327, 58878330, 58878370, 58878...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(AubU, 1856)</td>\n",
       "      <td>Auburn</td>\n",
       "      <td>Lee</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>1</td>\n",
       "      <td>Auburn, Alabama</td>\n",
       "      <td>(56844738, 56844741, 56845079, 56845082, 56845...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(AubU, 1856)</td>\n",
       "      <td>Florence</td>\n",
       "      <td>Lauderdale</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>0</td>\n",
       "      <td>Florence, Alabama</td>\n",
       "      <td>(56540270, 56540280, 56540305, 56540703, 56540...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(AubU, 1856)</td>\n",
       "      <td>Talladega</td>\n",
       "      <td>Talladega</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>0</td>\n",
       "      <td>Talladega, Alabama</td>\n",
       "      <td>(52315445, 52318540, 52326293, 52387532, 52398...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Experiment        Town      County    State  Treatment  \\\n",
       "0  (AlAMU, 1875)  Huntsville     Madison  Alabama          1   \n",
       "1  (AlAMU, 1875)  Montgomery  Montgomery  Alabama          0   \n",
       "3   (AubU, 1856)      Auburn         Lee  Alabama          1   \n",
       "4   (AubU, 1856)    Florence  Lauderdale  Alabama          0   \n",
       "5   (AubU, 1856)   Talladega   Talladega  Alabama          0   \n",
       "\n",
       "          Search_Place                                              Graph  \n",
       "0  Huntsville, Alabama  (56922621, 56923776, 56923781, 56923787, 56924...  \n",
       "1  Montgomery, Alabama  (58878326, 58878327, 58878330, 58878370, 58878...  \n",
       "3      Auburn, Alabama  (56844738, 56844741, 56845079, 56845082, 56845...  \n",
       "4    Florence, Alabama  (56540270, 56540280, 56540305, 56540703, 56540...  \n",
       "5   Talladega, Alabama  (52315445, 52318540, 52326293, 52387532, 52398...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.dropna()\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "986bf934-cc7b-421e-a304-3b2bc42b44af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nodes(graphs):\n",
    "    \n",
    "    all_features_list = []\n",
    "    \n",
    "    for G in graphs:\n",
    "        nodes = list(G.nodes)\n",
    "        degree_tensor = torch.tensor(list(G.degree()))\n",
    "        in_degree_tensor = torch.tensor(list(G.in_degree()))\n",
    "        all_features_tensor = torch.cat([degree_tensor.unsqueeze(1), in_degree_tensor.unsqueeze(1)], dim=1)\n",
    "        all_features_list.append(all_features_tensor)\n",
    "\n",
    "    combined_feature_tensor = torch.cat(all_features_list, dim=0)\n",
    "    return combined_feature_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "17c7d444-c658-4852-95c3-d89bf2ee72fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_edges(graphs):\n",
    "    \n",
    "    all_sparse_tensors = []\n",
    "    \n",
    "    for G in graphs:\n",
    "        adj_matrix = nx.to_scipy_sparse_array(G, dtype=float, format='csr')\n",
    "        sparse_tensor = torch.sparse_coo_tensor(adj_matrix.tocoo())\n",
    "        all_sparse_tensors.append(sparse_tensor)\n",
    "        \n",
    "    return all_sparse_tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "2953e0d8-34ad-4a24-9077-1f9243f63825",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_features = get_nodes(data[\"Graph\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "e27ada0a-b37f-4b0b-a1a0-ef6eaabd145c",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "sparse_coo_tensor(): argument 'size' (position 1) must be tuple of ints, but found element of type coo_array at pos 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[142], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m edge_features \u001b[38;5;241m=\u001b[39m \u001b[43mget_edges\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mGraph\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[141], line 7\u001b[0m, in \u001b[0;36mget_edges\u001b[1;34m(graphs)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m G \u001b[38;5;129;01min\u001b[39;00m graphs:\n\u001b[0;32m      6\u001b[0m     adj_matrix \u001b[38;5;241m=\u001b[39m nx\u001b[38;5;241m.\u001b[39mto_scipy_sparse_array(G, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 7\u001b[0m     sparse_tensor \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse_coo_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43madj_matrix\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtocoo\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m     all_sparse_tensors\u001b[38;5;241m.\u001b[39mappend(sparse_tensor)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m all_sparse_tensors\n",
      "\u001b[1;31mTypeError\u001b[0m: sparse_coo_tensor(): argument 'size' (position 1) must be tuple of ints, but found element of type coo_array at pos 0"
     ]
    }
   ],
   "source": [
    "edge_features = get_edges(data[\"Graph\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c92c93-bf52-4ea0-ba48-9dbaf7fd1f94",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f2f929bd-3903-4a4f-826d-e903e24c559d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionPool(nn.Module):\n",
    "      \"\"\" \n",
    "      \n",
    "      Attention-based pooling layer for graph classification.\n",
    "      Handles graphs of different sizes. \n",
    "      \n",
    "      \"\"\"\n",
    "\n",
    "      def __init__(self, units):\n",
    "        super(AttentionPool, self).__init__()\n",
    "        self.units = units\n",
    "        self.attention_dense = nn.Linear(units, units)\n",
    "        self.score_dense = nn.Linear(units, 1)\n",
    "\n",
    "      def forward(self, node_features, adj_matrix):\n",
    "        # Calculate attention scores\n",
    "        attention_inputs = self.attention_dense(node_features)\n",
    "        attention_logits = torch.matmul(attention_inputs, attention_inputs.t())\n",
    "        # Mask out self-attention\n",
    "        attention_logits -= torch.eye(attention_logits.size(0)) * 1e9\n",
    "        attention_weights = F.sigmoid(self.score_dense(attention_logits))\n",
    "\n",
    "        # Apply attention to node features\n",
    "        pooled_features = torch.matmul(attention_weights, node_features)\n",
    "\n",
    "        return pooled_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "02641356-029d-4d22-b896-1b8e754ee49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pool_features(node_features, edge_matrices):\n",
    "    pooled_features_list = []\n",
    "    for node_features_batch, adj_matrix_batch in zip(node_features, edge_matrices):\n",
    "        attention_pool = AttentionPool(units=128) # adjust?\n",
    "        pooled_features = attention_pool(node_features_batch, adj_matrix_batch)\n",
    "        pooled_features_list.append(pooled_features)\n",
    "    return pooled_features_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "1aa69fa9-343f-4f08-abb4-5707439a2bd3",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "linear(): argument 'input' (position 1) must be Tensor, not list",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[62], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m test_node \u001b[38;5;241m=\u001b[39m node_features[\u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m5\u001b[39m]\n\u001b[0;32m      2\u001b[0m test_edge \u001b[38;5;241m=\u001b[39m edge_features[\u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m5\u001b[39m]\n\u001b[1;32m----> 3\u001b[0m test_pool \u001b[38;5;241m=\u001b[39m \u001b[43mpool_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_node\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_edge\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[61], line 5\u001b[0m, in \u001b[0;36mpool_features\u001b[1;34m(node_features, edge_matrices)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m node_features_batch, adj_matrix_batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(node_features, edge_matrices):\n\u001b[0;32m      4\u001b[0m     attention_pool \u001b[38;5;241m=\u001b[39m AttentionPool(units\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m) \u001b[38;5;66;03m# adjust?\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m     pooled_features \u001b[38;5;241m=\u001b[39m \u001b[43mattention_pool\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode_features_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madj_matrix_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m     pooled_features_list\u001b[38;5;241m.\u001b[39mappend(pooled_features)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pooled_features_list\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[56], line 17\u001b[0m, in \u001b[0;36mAttentionPool.forward\u001b[1;34m(self, node_features, adj_matrix)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, node_features, adj_matrix):\n\u001b[0;32m     16\u001b[0m   \u001b[38;5;66;03m# Calculate attention scores\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m   attention_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention_dense\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode_features\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m   attention_logits \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(attention_inputs, attention_inputs\u001b[38;5;241m.\u001b[39mt())\n\u001b[0;32m     19\u001b[0m   \u001b[38;5;66;03m# Mask out self-attention\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: linear(): argument 'input' (position 1) must be Tensor, not list"
     ]
    }
   ],
   "source": [
    "test_node = node_features[0:5]\n",
    "test_edge = edge_features[0:5]\n",
    "test_pool = pool_features(test_node, test_edge)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b86b879-c9d6-4882-a423-5697d558a1c8",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b696706f-15e5-4162-9489-b976586d8a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNConv(nn.Module):\n",
    "     \"\"\"Graph Convolutional Layer\"\"\"\n",
    "       \n",
    "     def __init__(self, in_features, out_features):\n",
    "       super(GCNConv, self).__init__()\n",
    "       self.weight = nn.Linear(in_features, out_features)\n",
    "\n",
    "     def forward(self, node_features, adj_matrix):\n",
    "       # Normalize adjacency matrix\n",
    "       adj_matrix = F.normalize(adj_matrix, dim=1, p=1, eps=1e-9)\n",
    "       # Perform graph convolution\n",
    "       x = torch.matmul(adj_matrix, torch.matmul(node_features, self.weight))\n",
    "       return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fffa73a6-43ee-4d8c-a0e4-687ca4938f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphCNN(nn.Module):\n",
    "      \"\"\"Graphical CNN model for network classification.\"\"\"\n",
    "    \n",
    "      def __init__(self, in_features, hidden_features):\n",
    "        super(GraphCNN, self).__init__()\n",
    "        self.gcn1 = GCNConv(in_features, hidden_features)\n",
    "        self.gcn2 = GCNConv(hidden_features, hidden_features)\n",
    "        self.attention_pool = AttentionPool(hidden_features)\n",
    "        self.fc1 = nn.Linear(hidden_features, 64)\n",
    "        self.fc2 = nn.Linear(64, 2)\n",
    "    \n",
    "      def forward(self, node_features, adj_matrix):\n",
    "        x = self.gcn1(node_features, adj_matrix)\n",
    "        x = F.relu(x)\n",
    "        x = self.gcn2(x, adj_matrix)\n",
    "        x = F.relu(x)\n",
    "        x = self.attention_pool(x, adj_matrix)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, p=0.5)\n",
    "        output = F.sigmoid(self.fc2(x))\n",
    "        return output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24aecf4-881c-4c41-995e-968ac77c1c6f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
