{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "f687f71c-5518-4cfe-b7c9-7b06a057e5f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/cpsc477_sh2482/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import jsonlines\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.config.run_functions_eagerly(True)\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Flatten, Concatenate\n",
    "from tensorflow.keras.losses import binary_crossentropy, categorical_crossentropy\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from keras import ops\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.saving import register_keras_serializable\n",
    "\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "import string\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae2e5b78-8aa7-40d5-9479-40912fd40244",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"data/aspect-level-certainty.jsonl\"\n",
    "\n",
    "annotate_science = []\n",
    "\n",
    "with jsonlines.open(file) as f:\n",
    "    for line in f.iter():\n",
    "        annotate_science.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4993e90-76be-4b41-8461-876b6c749310",
   "metadata": {},
   "outputs": [],
   "source": [
    "finance = pd.read_csv(\"data/10K_sentiment_list.csv\")\n",
    "uncertain_finance = finance[finance[\"Uncertainty\"] != 0][\"Word\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81113678-ea15-4ba1-a3b3-e674edb0eab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restructure annotated data from scientific set\n",
    "label_codes = {\"NotPresent\": 0, \"Certain\": 1, \"Uncertain\": 2}\n",
    "dims = {\"Number\": None, \"Extent\": None, \"Probability\": None, \"Condition\": None, \"Suggestion\": None, \"Framing\": None}\n",
    "real_text = [x[\"finding\"] for x in annotate_science]\n",
    "real_labels = [[label_codes[x[\"aspect-level-certainty\"][dim]] for dim in dims] for x in annotate_science]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44dffda9-4ab4-443c-b1e1-e9c81ccf23b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 100\n",
    "vocab_size = 10000\n",
    "embedding_dim = 128\n",
    "num_label_dims = 6\n",
    "one_hot_len = 3\n",
    "label_values = [0,1,2]\n",
    "max_len = 64\n",
    "lstm_units = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fef0aadc-e944-4042-871c-d78f11b21f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    \"\"\"\n",
    "    Preprocesses text string by removing capitalization, punctuation, and stop words.\n",
    "\n",
    "    Args:\n",
    "        text: The text string to be preprocessed.\n",
    "\n",
    "    Returns:\n",
    "        The preprocessed text string.\n",
    "    \"\"\"\n",
    "\n",
    "    text = text.lower()\n",
    "    text = \"\".join(c for c in text if c not in string.punctuation)\n",
    "    stop_words = stopwords.words(\"english\")\n",
    "    text = \" \".join([word for word in text.split() if word not in stop_words])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f678982a-9198-4551-a6b9-1e2c32a714a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text_main, text_finance, max_len):\n",
    "    \"\"\"\n",
    "    Tokenizes preprocessed text string.\n",
    "\n",
    "    Args:\n",
    "        text: The preprocessed text string.\n",
    "        max_len: The maximum length for tokenized sequences\n",
    "\n",
    "    Returns:\n",
    "        A list of integers representing the tokenized text sequence.\n",
    "    \"\"\"\n",
    "    all_text = text_main + text_finance\n",
    "    \n",
    "    tokenizer = Tokenizer(num_words=vocab_size)\n",
    "    tokenizer.fit_on_texts(all_text)\n",
    "    vocab = tokenizer.word_index\n",
    "    \n",
    "    main_sequences = [tokenizer.texts_to_sequences([line])[0] for line in text_main] \n",
    "    for i in range(len(main_sequences)):\n",
    "        if len(main_sequences[i]) > max_len:\n",
    "            main_sequences[i] = main_sequences[i][:max_len]\n",
    "        else:\n",
    "            main_sequences[i] = tf.keras.utils.pad_sequences([main_sequences[i]], max_len, padding=\"post\")[0]\n",
    "\n",
    "    finance_tokens = [tokenizer.texts_to_sequences([word])[0] for word in text_finance]\n",
    "    \n",
    "    return main_sequences, finance_tokens, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "9e084625-de72-4088-b9d9-553f495a1afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_weight = 0.2\n",
    "\n",
    "@register_keras_serializable(name=\"generator_loss\")\n",
    "def generator_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculates a custom loss function.\n",
    "\n",
    "    Args:\n",
    "        y_true: The ground truth labels.\n",
    "        y_pred: The predicted labels.\n",
    "        domain_weight: The weight for finance specific uncertainty loss.\n",
    "\n",
    "    Returns:\n",
    "        The combined loss value.\n",
    "    \"\"\"\n",
    "    text_loss = categorical_crossentropy(y_true, y_pred)\n",
    "\n",
    "    # Make sure to use same embeddings as generator model\n",
    "    embedding_layer = generator.get_layer('embedding') \n",
    "    word_embedding_matrix = embedding_layer.get_weights()[0]\n",
    "\n",
    "    finance_word_embeddings = tf.nn.embedding_lookup(word_embedding_matrix, finance_token)\n",
    "    uncertainty_scores = tf.reduce_mean(finance_word_embeddings, axis=1)\n",
    "    uncertainty_loss = tf.reduce_mean(tf.abs(uncertainty_scores - y_true[:, 1]))\n",
    "    return text_loss + uncertainty_loss * domain_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "00dd27a1-95cb-4b3c-9a04-c1d7d3aef78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@register_keras_serializable(name=\"discriminator_loss\")\n",
    "def discriminator_loss(real_output, fake_output):\n",
    "    \"\"\"\n",
    "    Calculates loss function to distinguish between real and fake text.\n",
    "\n",
    "    Args:\n",
    "        real_output: The real text.\n",
    "        fake_output: The generated tex.\n",
    "\n",
    "    Returns:\n",
    "        The loss value.\n",
    "    \"\"\"\n",
    "    real_loss = binary_crossentropy(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = binary_crossentropy(tf.zeros_like(fake_output), fake_output)\n",
    "    label_loss = binary_crossentropy(real_output, fake_output)\n",
    "    return real_loss + fake_loss + label_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74100717-af81-421d-8bad-73fdd15129ba",
   "metadata": {},
   "source": [
    "### Build the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0ad4935c-8cc9-4e63-9e2e-4e762126e557",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_generator():\n",
    "    noise_input = Input(shape=(latent_dim,))\n",
    "    label_input = Input(shape=(one_hot_len,))\n",
    "    combined = Concatenate()([noise_input, label_input])\n",
    "    embedded = Embedding(vocab_size, embedding_dim, name=\"embedding\")(combined)\n",
    "    lstm1 = LSTM(256, return_sequences=True)(embedded)\n",
    "    lstm2 = LSTM(128)(lstm1)\n",
    "    output = Dense(max_len, activation='softmax')(lstm2)\n",
    "    model = Model(inputs=[noise_input, label_input], outputs=output)\n",
    "    model.compile(loss = generator_loss, optimizer=\"adam\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "81dd4ddb-e558-44c3-aea2-6b274dbed39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_discriminator():\n",
    "    text_input = Input(shape=(None,))\n",
    "    label_input = Input(shape=(one_hot_len,))\n",
    "    combined = Concatenate()([text_input, label_input])\n",
    "    embedded = Embedding(vocab_size, embedding_dim)(combined)\n",
    "    lstm1 = LSTM(256, return_sequences=True)(embedded)\n",
    "    lstm2 = LSTM(128)(lstm1)\n",
    "    sentiment_output = Dense(1, activation='sigmoid')(lstm2)\n",
    "    model = Model(inputs=[text_input, label_input], outputs=sentiment_output)\n",
    "    model.compile(loss = discriminator_loss, optimizer=\"adam\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c2670650-6331-444d-a840-866972244c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_combined():\n",
    "    noise_input = Input(shape=(latent_dim,))\n",
    "    sentiment_input = Input(shape=(one_hot_len,))\n",
    "    generated_text = generator([noise_input, sentiment_input])\n",
    "    validity = discriminator([generated_text, sentiment_input])\n",
    "    model = Model(inputs=[noise_input, sentiment_input], outputs=validity)\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7063d39a-bde7-466b-b32c-1b5f740dc6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_text_process = [preprocess(x) for x in real_text]\n",
    "finance_text_process = [preprocess(x) for x in uncertain_finance]\n",
    "\n",
    "real_text_token, finance_token, vocab = tokenize(real_text_process, finance_text_process, max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "64017be1-bf51-4a9b-90c8-ac2f3d8331fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_labels = [label_binarize(x, classes=[0,1,2]) for x in real_labels]\n",
    "\n",
    "dims[\"Number\"] = [x[0] for x in one_hot_labels]\n",
    "dims[\"Extent\"] = [x[1] for x in one_hot_labels]\n",
    "dims[\"Probability\"] = [x[2] for x in one_hot_labels]\n",
    "dims[\"Condition\"] = [x[3] for x in one_hot_labels]\n",
    "dims[\"Suggestion\"] = [x[4] for x in one_hot_labels]\n",
    "dims[\"Framing\"] = [x[5] for x in one_hot_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6295606d-cd0b-45be-a2b4-af79ad9b3b4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension: Number\n",
      "Epoch: 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cpsc477_sh2482/.conda/envs/myenv/lib/python3.12/site-packages/tensorflow/python/data/ops/structured_function.py:258: UserWarning: Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable_debug_mode()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 742ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cpsc477_sh2482/.conda/envs/myenv/lib/python3.12/site-packages/keras/src/losses/losses.py:22: SyntaxWarning: In loss categorical_crossentropy, expected y_pred.shape to be (batch_size, num_classes) with num_classes > 1. Received: y_pred.shape=(64, 1). Consider using 'binary_crossentropy' if you only have 2 classes.\n",
      "  return self.fn(y_true, y_pred, **self._fn_kwargs)\n",
      "/home/cpsc477_sh2482/.conda/envs/myenv/lib/python3.12/site-packages/keras/src/optimizers/base_optimizer.py:664: UserWarning: Gradients do not exist for variables ['embeddings', 'kernel', 'recurrent_kernel', 'bias', 'kernel', 'recurrent_kernel', 'bias', 'kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 1/27\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 755ms/step\n",
      "Batch: 2/27\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 743ms/step\n",
      "Batch: 3/27\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 757ms/step\n",
      "Batch: 4/27\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 750ms/step\n",
      "Batch: 5/27\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 752ms/step\n",
      "Batch: 6/27\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 764ms/step\n",
      "Batch: 7/27\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 752ms/step\n",
      "Batch: 8/27\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 759ms/step\n",
      "Batch: 9/27\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 747ms/step\n",
      "Batch: 10/27\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 776ms/step\n",
      "Batch: 11/27\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 763ms/step\n",
      "Batch: 12/27\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 754ms/step\n",
      "Batch: 13/27\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 759ms/step\n",
      "Batch: 14/27\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 777ms/step\n",
      "Batch: 15/27\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 757ms/step\n",
      "Batch: 16/27\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 765ms/step\n",
      "Batch: 17/27\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 759ms/step\n",
      "Batch: 18/27\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 759ms/step\n",
      "Batch: 19/27\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 747ms/step\n",
      "Batch: 20/27\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 746ms/step\n",
      "Batch: 21/27\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 746ms/step\n",
      "Batch: 22/27\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 754ms/step\n",
      "Batch: 23/27\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 752ms/step\n",
      "Batch: 24/27\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 753ms/step\n",
      "Batch: 25/27\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 780ms/step\n",
      "Batch: 26/27\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 755ms/step\n",
      "Batch: 27/27\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 1\n",
    "batch_size = 64\n",
    "\n",
    "num_batches = len(real_text_token) // batch_size\n",
    "\n",
    "real = np.ones((batch_size, 1))\n",
    "fake = np.zeros((batch_size, 1))\n",
    "\n",
    "all_d_loss = [[[0 for x in range(num_batches)] for y in range(num_epochs)] for z in range(num_label_dims)]\n",
    "all_gen_loss = [[[0 for x in range(num_batches)] for y in range(num_epochs)] for z in range(num_label_dims)]\n",
    "\n",
    "key_idx = 0\n",
    "#for key in dims.keys():\n",
    "for key in [\"Number\"]:\n",
    "    print(f\"Dimension: {key}\")\n",
    "    labels = dims[key]\n",
    "    key_idx += 1\n",
    "\n",
    "    generator = build_generator()\n",
    "    discriminator = build_discriminator()\n",
    "    combined = build_combined()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Epoch: {epoch+1}/{num_epochs}\")\n",
    "        for i in range(num_batches):\n",
    "        \n",
    "            start = i * batch_size\n",
    "            end = (i + 1) * batch_size\n",
    "            \n",
    "            real_data = tf.convert_to_tensor(real_text_token[start:end])\n",
    "            real_data_labels = tf.convert_to_tensor(labels[start:end])\n",
    "            \n",
    "            # Train Discriminator\n",
    "            noise_data = tf.convert_to_tensor(np.random.normal(size=(batch_size, latent_dim)))\n",
    "            generate_labels = tf.convert_to_tensor(np.random.randint(0, 2, size=(batch_size, one_hot_len)))\n",
    "    \n",
    "            generated_data = tf.convert_to_tensor(generator.predict([noise_data, generate_labels]))\n",
    "            real_data = Flatten()(real_data)\n",
    "            \n",
    "            real_loss = discriminator.train_on_batch([real_data, generate_labels], real)\n",
    "            fake_loss = discriminator.train_on_batch([generated_data, generate_labels], fake)\n",
    "            d_loss = 0.5 * np.add(real_loss, fake_loss)\n",
    "    \n",
    "            all_d_loss[key_idx][epoch][i] = d_loss\n",
    "            \n",
    "            # Train Generator        \n",
    "            gen_loss = combined.train_on_batch([noise_data, real_data_labels], real)\n",
    "    \n",
    "            all_gen_loss[key_idx][epoch][i] = gen_loss[0]\n",
    "\n",
    "            print(f\"Batch: {i+1}/{num_batches}\")\n",
    "    \n",
    "\n",
    "    f1 = f\"discriminator_{key}.keras\" \n",
    "    discriminator.save(f1) \n",
    "\n",
    "    f2 = f\"generator_{key}.keras\"  \n",
    "    generator.save(f2)\n",
    "\n",
    "    f3 = f\"combined_{key}.keras\"  \n",
    "    combined.save(f3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
