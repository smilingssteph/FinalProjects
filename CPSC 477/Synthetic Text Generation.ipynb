{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 403,
   "id": "f687f71c-5518-4cfe-b7c9-7b06a057e5f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\sh2482\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import jsonlines\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.config.run_functions_eagerly(True)\n",
    "\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Flatten, Lambda\n",
    "from tensorflow.keras.losses import binary_crossentropy, categorical_crossentropy\n",
    "from keras import ops\n",
    "\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "import string\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f08ccead-ae62-4425-a573-34a5cd32f6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"data/aspect-level-certainty.jsonl\"\n",
    "\n",
    "annotate_science = []\n",
    "\n",
    "with jsonlines.open(file) as f:\n",
    "    for line in f.iter():\n",
    "        annotate_science.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c4993e90-76be-4b41-8461-876b6c749310",
   "metadata": {},
   "outputs": [],
   "source": [
    "finance = pd.read_csv(\"data/10K_sentiment_list.csv\")\n",
    "uncertain_finance = finance[finance[\"Uncertainty\"] != 0][\"Word\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "81113678-ea15-4ba1-a3b3-e674edb0eab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restructure annotated data from scientific set\n",
    "label_codes = {\"NotPresent\": 0, \"Certain\": 1, \"Uncertain\": 2}\n",
    "dims = {\"Number\": None, \"Extent\": None, \"Probability\": None, \"Condition\": None, \"Suggestion\": None, \"Framing\": None}\n",
    "real_text = [x[\"finding\"] for x in annotate_science]\n",
    "real_labels = [[label_codes[x[\"aspect-level-certainty\"][dim]] for dim in dim_order] for x in annotate_science]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "id": "44dffda9-4ab4-443c-b1e1-e9c81ccf23b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 100\n",
    "embedding_dim = 128\n",
    "num_label_dims = 6\n",
    "one_hot_len = 3\n",
    "label_values = [0,1,2]\n",
    "max_len = 64\n",
    "lstm_units = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fef0aadc-e944-4042-871c-d78f11b21f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    \"\"\"\n",
    "    Preprocesses text string by removing capitalization, punctuation, and stop words.\n",
    "\n",
    "    Args:\n",
    "        text: The text string to be preprocessed.\n",
    "\n",
    "    Returns:\n",
    "        The preprocessed text string.\n",
    "    \"\"\"\n",
    "\n",
    "    text = text.lower()\n",
    "    text = \"\".join(c for c in text if c not in string.punctuation)\n",
    "    stop_words = stopwords.words(\"english\")\n",
    "    text = \" \".join([word for word in text.split() if word not in stop_words])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "id": "f678982a-9198-4551-a6b9-1e2c32a714a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text, max_len):\n",
    "    \"\"\"\n",
    "    Tokenizes preprocessed text string.\n",
    "\n",
    "    Args:\n",
    "        text: The preprocessed text string.\n",
    "        max_len: The maximum length for tokenized sequences\n",
    "\n",
    "    Returns:\n",
    "        A list of integers representing the tokenized text sequence.\n",
    "    \"\"\"\n",
    "\n",
    "    tokenizer = Tokenizer(num_words=vocab_size)\n",
    "    tokenizer.fit_on_texts([text])\n",
    "    sequences = tokenizer.texts_to_sequences([text])\n",
    "    if len(sequences) > max_len:\n",
    "        sequences = sequences[:max_len]\n",
    "    else:\n",
    "        sequences = tf.keras.utils.pad_sequences(sequences, max_len, padding=\"post\")\n",
    "    return sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "id": "9e084625-de72-4088-b9d9-553f495a1afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_weight = 0.2\n",
    "\n",
    "def generator_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculates a custom loss function.\n",
    "\n",
    "    Args:\n",
    "        y_true: The ground truth labels.\n",
    "        y_pred: The predicted labels.\n",
    "        domain_weight: The weight for finance specific uncertainty loss.\n",
    "\n",
    "    Returns:\n",
    "        The combined loss value.\n",
    "    \"\"\"\n",
    "    text_loss = categorical_crossentropy(y_true, y_pred)\n",
    "\n",
    "    # Make sure to use same embeddings as generator model\n",
    "    embedding_layer = generator.get_layer('embedding') \n",
    "    word_embedding_matrix = embedding_layer.get_weights()[0]\n",
    "\n",
    "    finance_word_embeddings = tf.nn.embedding_lookup(word_embedding_matrix, finance_token)\n",
    "    uncertainty_scores = tf.reduce_mean(finance_word_embeddings, axis=1)\n",
    "    uncertainty_loss = tf.reduce_mean(tf.abs(uncertainty_scores - y_true[:, 1]))\n",
    "    return text_loss + uncertainty_loss * domain_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "00dd27a1-95cb-4b3c-9a04-c1d7d3aef78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_loss(real_output, fake_output):\n",
    "    \"\"\"\n",
    "    Calculates loss function to distinguish between real and fake text.\n",
    "\n",
    "    Args:\n",
    "        real_output: The real text.\n",
    "        fake_output: The generated tex.\n",
    "\n",
    "    Returns:\n",
    "        The loss value.\n",
    "    \"\"\"\n",
    "    real_loss = binary_crossentropy(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = binary_crossentropy(tf.zeros_like(fake_output), fake_output)\n",
    "    label_loss = binary_crossentropy(real_output, fake_output)\n",
    "    return real_loss + fake_loss + label_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74100717-af81-421d-8bad-73fdd15129ba",
   "metadata": {},
   "source": [
    "### Build the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "id": "0ad4935c-8cc9-4e63-9e2e-4e762126e557",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_generator():\n",
    "    noise_input = Input(shape=(latent_dim,))\n",
    "    label_input = Input(shape=(one_hot_len,))\n",
    "    combined = Concatenate()([noise_input, label_input])\n",
    "    embedded = Embedding(vocab_size, embedding_dim, name=\"embedding\")(combined)\n",
    "    lstm1 = LSTM(256, return_sequences=True)(embedded)\n",
    "    lstm2 = LSTM(128)(lstm1)\n",
    "    output = Dense(max_len, activation='softmax')(lstm2)\n",
    "    model = Model(inputs=[noise_input, label_input], outputs=output)\n",
    "    model.compile(loss = generator_loss, optimizer=\"adam\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "id": "81dd4ddb-e558-44c3-aea2-6b274dbed39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_discriminator():\n",
    "    text_input = Input(shape=(None,))\n",
    "    label_input = Input(shape=(one_hot_len,))\n",
    "    combined = Concatenate()([text_input, label_input])\n",
    "    embedded = Embedding(vocab_size, embedding_dim)(combined)\n",
    "    lstm1 = LSTM(256, return_sequences=True)(embedded)\n",
    "    lstm2 = LSTM(128)(lstm1)\n",
    "    sentiment_output = Dense(1, activation='sigmoid')(lstm2)\n",
    "    model = Model(inputs=[text_input, label_input], outputs=sentiment_output)\n",
    "    model.compile(loss = discriminator_loss, optimizer=\"adam\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "id": "c2670650-6331-444d-a840-866972244c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_combined():\n",
    "    noise_input = Input(shape=(latent_dim,))\n",
    "    sentiment_input = Input(shape=(one_hot_len,))\n",
    "    generated_text = generator([noise_input, sentiment_input])\n",
    "    validity = discriminator([generated_text, sentiment_input])\n",
    "    model = Model(inputs=[noise_input, sentiment_input], outputs=validity)\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "id": "7063d39a-bde7-466b-b32c-1b5f740dc6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_text_process = [preprocess(x) for x in real_text]\n",
    "real_text_token = [tokenize(x, max_len) for x in real_text_process]\n",
    "\n",
    "finance_text_process = [preprocess(x) for x in uncertain_finance]\n",
    "finance_token = [tokenize(x, max_len) for x in finance_text_process]\n",
    "\n",
    "one_hot_labels = [label_binarize(x, classes=[0,1,2]) for x in real_labels]\n",
    "\n",
    "dims[\"Number\"] = [x[0] for x in one_hot_labels]\n",
    "dims[\"Extent\"] = [x[1] for x in one_hot_labels]\n",
    "dims[\"Probability\"] = [x[2] for x in one_hot_labels]\n",
    "dims[\"Condition\"] = [x[3] for x in one_hot_labels]\n",
    "dims[\"Suggestion\"] = [x[4] for x in one_hot_labels]\n",
    "dims[\"Framing\"] = [x[5] for x in one_hot_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "id": "6295606d-cd0b-45be-a2b4-af79ad9b3b4c",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "'[' was never closed (3801645415.py, line 36)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[500], line 36\u001b[1;36m\u001b[0m\n\u001b[1;33m    d_loss[\u001b[0m\n\u001b[1;37m          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m '[' was never closed\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 16\n",
    "batch_size = 64\n",
    "\n",
    "num_batches = len(real_text_token) // batch_size\n",
    "\n",
    "generator = build_generator()\n",
    "discriminator = build_discriminator()\n",
    "combined = build_combined()\n",
    "\n",
    "real = np.ones((batch_size, 1))\n",
    "fake = np.zeros((batch_size, 1))\n",
    "\n",
    "d_loss = [[0 for x in range(num_batches)] for y in range(num_epochs)]\n",
    "gen_loss = [[0 for x in range(num_batches)] for y in range(num_epochs)]\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i in range(num_batches):\n",
    "        \n",
    "        start = i * batch_size\n",
    "        end = (i + 1) * batch_size\n",
    "        \n",
    "        real_data = tf.convert_to_tensor(real_text_token[start:end])\n",
    "        real_data_labels = tf.convert_to_tensor(dims[\"Number\"][start:end])\n",
    "        \n",
    "        # Train Discriminator\n",
    "        noise_data = tf.convert_to_tensor(np.random.normal(size=(batch_size, latent_dim)))\n",
    "        generate_labels = tf.convert_to_tensor(np.random.randint(0, 2, size=(batch_size, one_hot_len)))\n",
    "\n",
    "        generated_data = tf.convert_to_tensor(generator.predict([noise_data, generate_labels]))\n",
    "        real_data = Flatten()(real_data)\n",
    "        \n",
    "        real_loss = discriminator.train_on_batch([real_data, generate_labels], real)\n",
    "        fake_loss = discriminator.train_on_batch([generated_data, generate_labels], fake)\n",
    "        d_loss = 0.5 * np.add(real_loss, fake_loss)\n",
    "\n",
    "        d_loss[epoch][i] = d_loss\n",
    "        \n",
    "        # Train Generator        \n",
    "        gen_loss = combined.train_on_batch([noise_data, real_data_labels], real)\n",
    "\n",
    "        gen_loss[epoch][i] = gen_loss\n",
    "    \n",
    "        # Print training progress\n",
    "    print(f\"Epoch: {epoch+1}/{num_epochs}, Discriminator Loss: {d_loss}, Generator Loss: {gen_loss[0]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
