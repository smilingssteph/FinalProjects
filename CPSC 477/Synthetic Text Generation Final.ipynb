{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f687f71c-5518-4cfe-b7c9-7b06a057e5f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-07 15:23:37.270507: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-05-07 15:23:37.470735: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-05-07 15:23:38.461727: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-07 15:23:42.102181: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/cpsc477_sh2482/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import jsonlines\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.config.run_functions_eagerly(True)\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Flatten, Concatenate, ZeroPadding1D, Layer\n",
    "from tensorflow.keras.losses import binary_crossentropy, categorical_crossentropy\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from keras import ops\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.saving import register_keras_serializable\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "import string\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "194b52de-fe76-4eb2-bae8-9b2d49d7362b",
   "metadata": {},
   "source": [
    "## Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae2e5b78-8aa7-40d5-9479-40912fd40244",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"data/aspect-level-certainty.jsonl\"\n",
    "\n",
    "annotate_science = []\n",
    "\n",
    "with jsonlines.open(file) as f:\n",
    "    for line in f.iter():\n",
    "        annotate_science.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4993e90-76be-4b41-8461-876b6c749310",
   "metadata": {},
   "outputs": [],
   "source": [
    "finance = pd.read_csv(\"data/10K_sentiment_list.csv\")\n",
    "uncertain_finance = finance[finance[\"Uncertainty\"] != 0][\"Word\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81113678-ea15-4ba1-a3b3-e674edb0eab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restructure annotated data from scientific set\n",
    "label_codes = {\"NotPresent\": 0, \"Certain\": 1, \"Uncertain\": 2}\n",
    "dims = {\"Number\": None, \"Extent\": None, \"Probability\": None, \"Condition\": None, \"Suggestion\": None, \"Framing\": None}\n",
    "real_text = [x[\"finding\"] for x in annotate_science]\n",
    "real_labels = [[label_codes[x[\"aspect-level-certainty\"][dim]] for dim in dims] for x in annotate_science]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44dffda9-4ab4-443c-b1e1-e9c81ccf23b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 100\n",
    "vocab_size = 10000\n",
    "embedding_dim = 128\n",
    "num_label_dims = 6\n",
    "label_dim = 3\n",
    "label_values = [0,1,2]\n",
    "max_len = 64\n",
    "start_token = \"<START>\"\n",
    "stop_token = \"<STOP>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fef0aadc-e944-4042-871c-d78f11b21f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    \"\"\"\n",
    "    Preprocesses text string by removing capitalization, punctuation, and stop words.\n",
    "\n",
    "    Args:\n",
    "        text: The text string to be preprocessed.\n",
    "\n",
    "    Returns:\n",
    "        The preprocessed text string.\n",
    "    \"\"\"\n",
    "\n",
    "    text = text.lower()\n",
    "    text = \"\".join(c for c in text if c not in string.punctuation)\n",
    "    stop_words = stopwords.words(\"english\")\n",
    "    text = \" \".join([word for word in text.split() if word not in stop_words])\n",
    "    text = start_token + \" \" + text + \" \" + stop_token\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f678982a-9198-4551-a6b9-1e2c32a714a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text_main, text_finance, max_len):\n",
    "    \"\"\"\n",
    "    Tokenizes preprocessed text string.\n",
    "\n",
    "    Args:\n",
    "        text: The preprocessed text string.\n",
    "        max_len: The maximum length for tokenized sequences\n",
    "\n",
    "    Returns:\n",
    "        A list of integers representing the tokenized text sequence.\n",
    "    \"\"\"\n",
    "    all_text = text_main + text_finance\n",
    "    \n",
    "    tokenizer = Tokenizer(num_words=vocab_size)\n",
    "    tokenizer.fit_on_texts(all_text)\n",
    "    vocab = tokenizer.word_index\n",
    "    \n",
    "    main_sequences = [tokenizer.texts_to_sequences([line])[0] for line in text_main] \n",
    "    for i in range(len(main_sequences)):\n",
    "        if len(main_sequences[i]) > max_len:\n",
    "            main_sequences[i] = main_sequences[i][:max_len]\n",
    "        else:\n",
    "            main_sequences[i] = tf.keras.utils.pad_sequences([main_sequences[i]], max_len, padding=\"post\")[0]\n",
    "\n",
    "    finance_tokens = [tokenizer.texts_to_sequences([word])[0] for word in text_finance]\n",
    "    \n",
    "    return main_sequences, finance_tokens, vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea7ef11-07b3-496e-beeb-3acb678cb088",
   "metadata": {},
   "source": [
    "## Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e084625-de72-4088-b9d9-553f495a1afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_weight = 0.3\n",
    "@keras.saving.register_keras_serializable()\n",
    "def generator_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculates a custom loss function incorporating uncertainty lexicon.\n",
    "\n",
    "    Args:\n",
    "        y_true: The ground truth labels.\n",
    "        y_pred: The predicted labels.\n",
    "\n",
    "    Returns:\n",
    "        The combined loss value.\n",
    "    \"\"\"\n",
    "    text_loss = categorical_crossentropy(y_true, y_pred)\n",
    "\n",
    "    # Extract uncertainty words embeddings from the embedding layer\n",
    "    uncertainty_word_embeddings = tf.nn.embedding_lookup(generator.get_layer('embedding').weights[0], finance_token)\n",
    "\n",
    "    # Compute similarity between predicted embeddings and uncertainty word embeddings\n",
    "    predicted_word_embeddings = tf.nn.embedding_lookup(generator.get_layer('embedding').weights[0], tf.argmax(y_pred, axis=-1))\n",
    "    similarity = tf.matmul(predicted_word_embeddings, uncertainty_word_embeddings, transpose_b=True)\n",
    "\n",
    "    # Calculate uncertainty loss based on similarity\n",
    "    uncertainty_loss = tf.reduce_mean(tf.reduce_max(similarity, axis=-1))\n",
    "\n",
    "    # Adjust the weights of the text loss and uncertainty loss\n",
    "    combined_loss = text_loss + uncertainty_loss * domain_weight\n",
    "\n",
    "    return combined_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "00dd27a1-95cb-4b3c-9a04-c1d7d3aef78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@keras.saving.register_keras_serializable()\n",
    "def discriminator_loss(real_output, fake_output):\n",
    "    \"\"\"\n",
    "    Calculates loss function to distinguish between real and fake text.\n",
    "\n",
    "    Args:\n",
    "        real_output: The real text.\n",
    "        fake_output: The generated tex.\n",
    "\n",
    "    Returns:\n",
    "        The loss value.\n",
    "    \"\"\"\n",
    "    real_loss = binary_crossentropy(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = binary_crossentropy(tf.zeros_like(fake_output), fake_output)\n",
    "    label_loss = binary_crossentropy(real_output, fake_output)\n",
    "    return real_loss + fake_loss + label_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ad4935c-8cc9-4e63-9e2e-4e762126e557",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'keras' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;129m@keras\u001b[39m\u001b[38;5;241m.\u001b[39msaving\u001b[38;5;241m.\u001b[39mregister_keras_serializable()\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mGenerator\u001b[39;00m(keras\u001b[38;5;241m.\u001b[39mModel):\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, vocab_size, embedding_dim, latent_dim, label_dim, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m      4\u001b[0m         \u001b[38;5;28msuper\u001b[39m(Generator, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'keras' is not defined"
     ]
    }
   ],
   "source": [
    "@keras.saving.register_keras_serializable()\n",
    "class Generator(keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, latent_dim, label_dim, **kwargs):\n",
    "        super(Generator, self).__init__(**kwargs)\n",
    "        self.embedding = Embedding(vocab_size, embedding_dim, name=\"embedding\")\n",
    "        self.lstm = LSTM(256, return_sequences=True)\n",
    "        self.dense = Dense(vocab_size, activation=\"softmax\")\n",
    "        self.label_embedding = Dense(embedding_dim)\n",
    "    \n",
    "    def call(self, inputs, training=None):\n",
    "        noise, label = inputs\n",
    "        noise = tf.reshape(noise, (-1, latent_dim)) \n",
    "        label_embedding = self.label_embedding(label)\n",
    "        embedded_noise = self.embedding(noise)\n",
    "        label_embedding_repeated = tf.tile(tf.expand_dims(label_embedding, 1), [1, tf.shape(embedded_noise)[1], 1])\n",
    "        combined_input = keras.layers.concatenate([embedded_noise, label_embedding_repeated])\n",
    "        output = self.lstm(combined_input)\n",
    "        output = self.dense(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "81dd4ddb-e558-44c3-aea2-6b274dbed39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@keras.saving.register_keras_serializable()\n",
    "class Discriminator(keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim, label_dim, **kwargs):\n",
    "    super(Discriminator, self).__init__(**kwargs)\n",
    "    self.embedding = Embedding(vocab_size, embedding_dim)\n",
    "    self.lstm = LSTM(256)\n",
    "    self.label_embedding = Dense(embedding_dim)\n",
    "    self.dense = Dense(1, activation=\"sigmoid\")\n",
    "\n",
    "  def call(self, inputs, training=None):\n",
    "    text, label = inputs\n",
    "    label_embedding = self.label_embedding(label)\n",
    "    label_embedding_reshaped = tf.expand_dims(label_embedding, axis=1) \n",
    "    label_embedding_reshaped = tf.tile(label_embedding_reshaped, [1, max_len, 1]) \n",
    "    embedded_text = self.embedding(text)\n",
    "    combined_input = keras.layers.concatenate([embedded_text, label_embedding_reshaped])\n",
    "    output = self.lstm(combined_input)\n",
    "    output = self.dense(output)\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a9ee68d3-75d3-491e-914b-a61c34efc85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@keras.saving.register_keras_serializable()\n",
    "def train_step(real_data, labels, noise):\n",
    "    with tf.GradientTape() as disc_tape, tf.GradientTape() as gen_tape:\n",
    "        # Generate fake data\n",
    "        fake_data_logits = generator([noise, labels], training=True)\n",
    "\n",
    "        # Generate sequences from logits\n",
    "        fake_data_sequences = tf.TensorArray(tf.int32, size=fake_data_logits.shape[0], dynamic_size=False)\n",
    "        for i in tf.range(fake_data_logits.shape[0]):\n",
    "            generated_sequence = []\n",
    "            for _ in range(max_len):\n",
    "                logits = fake_data_logits[i:i+1, _, :]\n",
    "                generated_token = tf.random.categorical(logits, num_samples=1)\n",
    "                generated_sequence.append(generated_token[0, 0].numpy())\n",
    "            fake_data_sequences = fake_data_sequences.write(i, generated_sequence)\n",
    "        fake_data_sequences = fake_data_sequences.stack()\n",
    "\n",
    "        # Train Discriminator\n",
    "        real_output = discriminator([real_data, labels], training=True)\n",
    "        fake_output = discriminator([fake_data_sequences, labels], training=True)\n",
    "        disc_loss = discriminator_loss(tf.ones_like(real_output), real_output) + discriminator_loss(tf.zeros_like(fake_output), fake_output)\n",
    "\n",
    "        # Train Generator\n",
    "        gen_output = discriminator([fake_data_sequences, labels], training=True)\n",
    "        gen_loss = generator_loss(tf.ones_like(gen_output), gen_output)\n",
    "\n",
    "    # Compute gradients and apply optimizer updates for Discriminator\n",
    "    gradients_of_disc = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_disc, discriminator.trainable_variables))\n",
    "\n",
    "    # Compute gradients and apply optimizer updates for Generator\n",
    "    gradients_of_gen = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_gen, generator.trainable_variables))\n",
    "\n",
    "    return disc_loss, gen_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74100717-af81-421d-8bad-73fdd15129ba",
   "metadata": {},
   "source": [
    "## Build and Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7063d39a-bde7-466b-b32c-1b5f740dc6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_text_process = [preprocess(x) for x in real_text]\n",
    "finance_text_process = [preprocess(x) for x in uncertain_finance]\n",
    "\n",
    "real_text_token, finance_token, vocab = tokenize(real_text_process, finance_text_process, max_len)\n",
    "vocab_search = {value: key for key, value in vocab.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "64017be1-bf51-4a9b-90c8-ac2f3d8331fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_labels = [label_binarize(x, classes=[0,1,2]) for x in real_labels]\n",
    "\n",
    "dims[\"Number\"] = [x[0] for x in one_hot_labels]\n",
    "dims[\"Extent\"] = [x[1] for x in one_hot_labels]\n",
    "dims[\"Probability\"] = [x[2] for x in one_hot_labels]\n",
    "dims[\"Condition\"] = [x[3] for x in one_hot_labels]\n",
    "dims[\"Suggestion\"] = [x[4] for x in one_hot_labels]\n",
    "dims[\"Framing\"] = [x[5] for x in one_hot_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63fc5729-ab94-4298-857e-545cfe18e7f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension: Number\n",
      "Epoch: 1/15\n",
      "Batch: 1/27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1915763/2397356603.py:14: SyntaxWarning: In loss categorical_crossentropy, expected y_pred.shape to be (batch_size, num_classes) with num_classes > 1. Received: y_pred.shape=(64, 1). Consider using 'binary_crossentropy' if you only have 2 classes.\n",
      "  text_loss = categorical_crossentropy(y_true, y_pred)\n",
      "/home/cpsc477_sh2482/.conda/envs/myenv/lib/python3.12/site-packages/keras/src/optimizers/base_optimizer.py:664: UserWarning: Gradients do not exist for variables ['kernel', 'recurrent_kernel', 'bias', 'kernel', 'bias', 'kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 2/27\n",
      "Batch: 3/27\n",
      "Batch: 4/27\n",
      "Batch: 5/27\n",
      "Batch: 6/27\n",
      "Batch: 7/27\n",
      "Batch: 8/27\n",
      "Batch: 9/27\n",
      "Batch: 10/27\n",
      "Batch: 11/27\n",
      "Batch: 12/27\n",
      "Batch: 13/27\n",
      "Batch: 14/27\n",
      "Batch: 15/27\n",
      "Batch: 16/27\n",
      "Batch: 17/27\n",
      "Batch: 18/27\n",
      "Batch: 19/27\n",
      "Batch: 20/27\n",
      "Batch: 21/27\n",
      "Batch: 22/27\n",
      "Batch: 23/27\n",
      "Batch: 24/27\n",
      "Batch: 25/27\n",
      "Batch: 26/27\n",
      "Batch: 27/27\n",
      "Epoch: 2/15\n",
      "Batch: 1/27\n",
      "Batch: 2/27\n",
      "Batch: 3/27\n",
      "Batch: 4/27\n",
      "Batch: 5/27\n",
      "Batch: 6/27\n",
      "Batch: 7/27\n",
      "Batch: 8/27\n",
      "Batch: 9/27\n",
      "Batch: 10/27\n",
      "Batch: 11/27\n",
      "Batch: 12/27\n",
      "Batch: 13/27\n",
      "Batch: 14/27\n",
      "Batch: 15/27\n",
      "Batch: 16/27\n",
      "Batch: 17/27\n",
      "Batch: 18/27\n",
      "Batch: 19/27\n",
      "Batch: 20/27\n",
      "Batch: 21/27\n",
      "Batch: 22/27\n",
      "Batch: 23/27\n",
      "Batch: 24/27\n",
      "Batch: 25/27\n",
      "Batch: 26/27\n",
      "Batch: 27/27\n",
      "Epoch: 3/15\n",
      "Batch: 1/27\n",
      "Batch: 2/27\n",
      "Batch: 3/27\n",
      "Batch: 4/27\n",
      "Batch: 5/27\n",
      "Batch: 6/27\n",
      "Batch: 7/27\n",
      "Batch: 8/27\n",
      "Batch: 9/27\n",
      "Batch: 10/27\n",
      "Batch: 11/27\n",
      "Batch: 12/27\n",
      "Batch: 13/27\n",
      "Batch: 14/27\n",
      "Batch: 15/27\n",
      "Batch: 16/27\n",
      "Batch: 17/27\n",
      "Batch: 18/27\n",
      "Batch: 19/27\n",
      "Batch: 20/27\n",
      "Batch: 21/27\n",
      "Batch: 22/27\n",
      "Batch: 23/27\n",
      "Batch: 24/27\n",
      "Batch: 25/27\n",
      "Batch: 26/27\n",
      "Batch: 27/27\n",
      "Epoch: 4/15\n",
      "Batch: 1/27\n",
      "Batch: 2/27\n",
      "Batch: 3/27\n",
      "Batch: 4/27\n",
      "Batch: 5/27\n",
      "Batch: 6/27\n",
      "Batch: 7/27\n",
      "Batch: 8/27\n",
      "Batch: 9/27\n",
      "Batch: 10/27\n",
      "Batch: 11/27\n",
      "Batch: 12/27\n",
      "Batch: 17/27\n",
      "Batch: 18/27\n",
      "Batch: 19/27\n",
      "Batch: 20/27\n",
      "Batch: 21/27\n",
      "Batch: 22/27\n",
      "Batch: 23/27\n",
      "Batch: 24/27\n",
      "Batch: 25/27\n",
      "Batch: 26/27\n",
      "Batch: 27/27\n",
      "Epoch: 5/15\n",
      "Batch: 1/27\n",
      "Batch: 2/27\n",
      "Batch: 3/27\n",
      "Batch: 4/27\n",
      "Batch: 5/27\n",
      "Batch: 6/27\n",
      "Batch: 7/27\n",
      "Batch: 8/27\n",
      "Batch: 9/27\n",
      "Batch: 10/27\n",
      "Batch: 11/27\n",
      "Batch: 12/27\n",
      "Batch: 13/27\n",
      "Batch: 14/27\n",
      "Batch: 15/27\n",
      "Batch: 16/27\n",
      "Batch: 17/27\n",
      "Batch: 18/27\n",
      "Batch: 19/27\n",
      "Batch: 20/27\n",
      "Batch: 21/27\n",
      "Batch: 22/27\n",
      "Batch: 23/27\n",
      "Batch: 24/27\n",
      "Batch: 25/27\n",
      "Batch: 26/27\n",
      "Batch: 27/27\n",
      "Epoch: 6/15\n",
      "Batch: 1/27\n",
      "Batch: 2/27\n",
      "Batch: 3/27\n",
      "Batch: 4/27\n",
      "Batch: 5/27\n",
      "Batch: 6/27\n",
      "Batch: 7/27\n",
      "Batch: 8/27\n",
      "Batch: 9/27\n",
      "Batch: 10/27\n",
      "Batch: 11/27\n",
      "Batch: 12/27\n",
      "Batch: 13/27\n",
      "Batch: 14/27\n",
      "Batch: 15/27\n",
      "Batch: 16/27\n",
      "Batch: 17/27\n",
      "Batch: 18/27\n",
      "Batch: 19/27\n",
      "Batch: 20/27\n",
      "Batch: 21/27\n",
      "Batch: 22/27\n",
      "Batch: 23/27\n",
      "Batch: 24/27\n",
      "Batch: 25/27\n",
      "Batch: 26/27\n",
      "Batch: 27/27\n",
      "Epoch: 7/15\n",
      "Batch: 1/27\n",
      "Batch: 2/27\n",
      "Batch: 3/27\n",
      "Batch: 4/27\n",
      "Batch: 5/27\n",
      "Batch: 6/27\n",
      "Batch: 7/27\n",
      "Batch: 8/27\n",
      "Batch: 9/27\n",
      "Batch: 10/27\n",
      "Batch: 11/27\n",
      "Batch: 12/27\n",
      "Batch: 13/27\n",
      "Batch: 14/27\n",
      "Batch: 15/27\n",
      "Batch: 16/27\n",
      "Batch: 17/27\n",
      "Batch: 18/27\n",
      "Batch: 22/27\n",
      "Batch: 23/27\n",
      "Batch: 24/27\n",
      "Batch: 25/27\n",
      "Batch: 26/27\n",
      "Batch: 27/27\n",
      "Epoch: 8/15\n",
      "Batch: 1/27\n",
      "Batch: 2/27\n",
      "Batch: 3/27\n",
      "Batch: 4/27\n",
      "Batch: 5/27\n",
      "Batch: 6/27\n",
      "Batch: 7/27\n",
      "Batch: 8/27\n",
      "Batch: 9/27\n",
      "Batch: 10/27\n",
      "Batch: 11/27\n",
      "Batch: 12/27\n",
      "Batch: 13/27\n",
      "Batch: 14/27\n",
      "Batch: 15/27\n",
      "Batch: 16/27\n",
      "Batch: 17/27\n",
      "Batch: 18/27\n",
      "Batch: 19/27\n",
      "Batch: 20/27\n",
      "Batch: 21/27\n",
      "Batch: 22/27\n",
      "Batch: 23/27\n",
      "Batch: 24/27\n",
      "Batch: 25/27\n",
      "Batch: 26/27\n",
      "Batch: 27/27\n",
      "Epoch: 9/15\n",
      "Batch: 1/27\n",
      "Batch: 2/27\n",
      "Batch: 3/27\n",
      "Batch: 4/27\n",
      "Batch: 5/27\n",
      "Batch: 6/27\n",
      "Batch: 7/27\n",
      "Batch: 8/27\n",
      "Batch: 9/27\n",
      "Batch: 10/27\n",
      "Batch: 11/27\n",
      "Batch: 12/27\n",
      "Batch: 13/27\n",
      "Batch: 14/27\n",
      "Batch: 15/27\n",
      "Batch: 16/27\n",
      "Batch: 17/27\n",
      "Batch: 18/27\n",
      "Batch: 19/27\n",
      "Batch: 20/27\n",
      "Batch: 21/27\n",
      "Batch: 22/27\n",
      "Batch: 23/27\n",
      "Batch: 24/27\n",
      "Batch: 25/27\n",
      "Batch: 26/27\n",
      "Batch: 27/27\n",
      "Epoch: 10/15\n",
      "Batch: 1/27\n",
      "Batch: 2/27\n",
      "Batch: 3/27\n",
      "Batch: 4/27\n",
      "Batch: 5/27\n",
      "Batch: 6/27\n",
      "Batch: 7/27\n",
      "Batch: 8/27\n",
      "Batch: 9/27\n",
      "Batch: 10/27\n",
      "Batch: 11/27\n",
      "Batch: 12/27\n",
      "Batch: 13/27\n",
      "Batch: 14/27\n",
      "Batch: 15/27\n",
      "Batch: 16/27\n",
      "Batch: 17/27\n",
      "Batch: 18/27\n",
      "Batch: 19/27\n",
      "Batch: 20/27\n",
      "Batch: 21/27\n",
      "Batch: 22/27\n",
      "Batch: 23/27\n",
      "Batch: 24/27\n",
      "Batch: 25/27\n",
      "Batch: 26/27\n",
      "Batch: 27/27\n",
      "Epoch: 11/15\n",
      "Batch: 1/27\n",
      "Batch: 2/27\n",
      "Batch: 3/27\n",
      "Batch: 4/27\n",
      "Batch: 5/27\n",
      "Batch: 6/27\n",
      "Batch: 7/27\n",
      "Batch: 8/27\n",
      "Batch: 9/27\n",
      "Batch: 10/27\n",
      "Batch: 11/27\n",
      "Batch: 12/27\n",
      "Batch: 13/27\n",
      "Batch: 14/27\n",
      "Batch: 15/27\n",
      "Batch: 16/27\n",
      "Batch: 17/27\n",
      "Batch: 18/27\n",
      "Batch: 19/27\n",
      "Batch: 20/27\n",
      "Batch: 21/27\n",
      "Batch: 22/27\n",
      "Batch: 23/27\n",
      "Batch: 24/27\n",
      "Batch: 25/27\n",
      "Batch: 26/27\n",
      "Batch: 27/27\n",
      "Epoch: 12/15\n",
      "Batch: 1/27\n",
      "Batch: 2/27\n",
      "Batch: 3/27\n",
      "Batch: 4/27\n",
      "Batch: 5/27\n",
      "Batch: 6/27\n",
      "Batch: 7/27\n",
      "Batch: 8/27\n",
      "Batch: 9/27\n",
      "Batch: 10/27\n",
      "Batch: 11/27\n",
      "Batch: 12/27\n",
      "Batch: 13/27\n",
      "Batch: 14/27\n",
      "Batch: 15/27\n",
      "Batch: 16/27\n",
      "Batch: 17/27\n",
      "Batch: 18/27\n",
      "Batch: 19/27\n",
      "Batch: 20/27\n",
      "Batch: 21/27\n",
      "Batch: 22/27\n",
      "Batch: 23/27\n",
      "Batch: 24/27\n",
      "Batch: 25/27\n",
      "Batch: 26/27\n",
      "Batch: 27/27\n",
      "Epoch: 13/15\n",
      "Batch: 1/27\n",
      "Batch: 2/27\n",
      "Batch: 3/27\n",
      "Batch: 4/27\n",
      "Batch: 5/27\n",
      "Batch: 6/27\n",
      "Batch: 7/27\n",
      "Batch: 8/27\n",
      "Batch: 9/27\n",
      "Batch: 10/27\n",
      "Batch: 11/27\n",
      "Batch: 12/27\n",
      "Batch: 13/27\n",
      "Batch: 14/27\n",
      "Batch: 15/27\n",
      "Batch: 16/27\n",
      "Batch: 17/27\n",
      "Batch: 18/27\n",
      "Batch: 19/27\n",
      "Batch: 20/27\n",
      "Batch: 21/27\n",
      "Batch: 22/27\n",
      "Batch: 23/27\n",
      "Batch: 24/27\n",
      "Batch: 25/27\n",
      "Batch: 26/27\n",
      "Batch: 27/27\n",
      "Epoch: 14/15\n",
      "Batch: 1/27\n",
      "Batch: 2/27\n",
      "Batch: 3/27\n",
      "Batch: 4/27\n",
      "Batch: 5/27\n",
      "Batch: 6/27\n",
      "Batch: 7/27\n",
      "Batch: 8/27\n",
      "Batch: 9/27\n",
      "Batch: 10/27\n",
      "Batch: 11/27\n",
      "Batch: 12/27\n",
      "Batch: 13/27\n",
      "Batch: 14/27\n",
      "Batch: 15/27\n",
      "Batch: 16/27\n",
      "Batch: 17/27\n",
      "Batch: 18/27\n",
      "Batch: 19/27\n",
      "Batch: 20/27\n",
      "Batch: 21/27\n",
      "Batch: 22/27\n",
      "Batch: 23/27\n",
      "Batch: 24/27\n",
      "Batch: 25/27\n",
      "Batch: 26/27\n",
      "Batch: 27/27\n",
      "Epoch: 15/15\n",
      "Batch: 1/27\n",
      "Batch: 2/27\n",
      "Batch: 3/27\n",
      "Batch: 4/27\n",
      "Batch: 5/27\n",
      "Batch: 6/27\n",
      "Batch: 7/27\n",
      "Batch: 8/27\n",
      "Batch: 9/27\n",
      "Batch: 10/27\n",
      "Batch: 11/27\n",
      "Batch: 12/27\n",
      "Batch: 13/27\n",
      "Batch: 14/27\n",
      "Batch: 15/27\n",
      "Batch: 16/27\n",
      "Batch: 17/27\n",
      "Batch: 18/27\n",
      "Batch: 19/27\n",
      "Batch: 20/27\n",
      "Batch: 21/27\n",
      "Batch: 22/27\n",
      "Batch: 23/27\n",
      "Batch: 24/27\n",
      "Batch: 25/27\n",
      "Batch: 26/27\n",
      "Batch: 27/27\n",
      "Dimension: Extent\n",
      "Epoch: 1/15\n",
      "Batch: 1/27\n",
      "Batch: 2/27\n",
      "Batch: 3/27\n",
      "Batch: 4/27\n",
      "Batch: 5/27\n",
      "Batch: 6/27\n",
      "Batch: 7/27\n",
      "Batch: 8/27\n",
      "Batch: 9/27\n",
      "Batch: 10/27\n",
      "Batch: 11/27\n",
      "Batch: 12/27\n",
      "Batch: 13/27\n",
      "Batch: 14/27\n",
      "Batch: 15/27\n",
      "Batch: 16/27\n",
      "Batch: 17/27\n",
      "Batch: 18/27\n",
      "Batch: 19/27\n",
      "Batch: 20/27\n",
      "Batch: 21/27\n",
      "Batch: 22/27\n",
      "Batch: 23/27\n",
      "Batch: 24/27\n",
      "Batch: 25/27\n",
      "Batch: 26/27\n",
      "Batch: 27/27\n",
      "Epoch: 2/15\n",
      "Batch: 1/27\n",
      "Batch: 2/27\n",
      "Batch: 3/27\n",
      "Batch: 4/27\n",
      "Batch: 5/27\n",
      "Batch: 6/27\n",
      "Batch: 7/27\n",
      "Batch: 8/27\n",
      "Batch: 9/27\n",
      "Batch: 10/27\n",
      "Batch: 11/27\n",
      "Batch: 12/27\n",
      "Batch: 13/27\n",
      "Batch: 14/27\n",
      "Batch: 15/27\n",
      "Batch: 16/27\n",
      "Batch: 17/27\n",
      "Batch: 18/27\n",
      "Batch: 19/27\n",
      "Batch: 20/27\n",
      "Batch: 21/27\n",
      "Batch: 22/27\n",
      "Batch: 23/27\n",
      "Batch: 24/27\n",
      "Batch: 25/27\n",
      "Batch: 26/27\n",
      "Batch: 27/27\n",
      "Epoch: 3/15\n",
      "Batch: 1/27\n",
      "Batch: 2/27\n",
      "Batch: 3/27\n",
      "Batch: 4/27\n",
      "Batch: 5/27\n",
      "Batch: 6/27\n",
      "Batch: 7/27\n",
      "Batch: 8/27\n",
      "Batch: 9/27\n",
      "Batch: 10/27\n",
      "Batch: 11/27\n",
      "Batch: 17/27\n",
      "Batch: 18/27\n",
      "Batch: 19/27\n",
      "Batch: 20/27\n",
      "Batch: 21/27\n",
      "Batch: 25/27\n",
      "Batch: 26/27\n",
      "Batch: 27/27\n",
      "Epoch: 4/15\n",
      "Batch: 1/27\n",
      "Batch: 2/27\n",
      "Batch: 3/27\n",
      "Batch: 4/27\n",
      "Batch: 5/27\n",
      "Batch: 6/27\n",
      "Batch: 7/27\n",
      "Batch: 8/27\n",
      "Batch: 9/27\n",
      "Batch: 10/27\n",
      "Batch: 11/27\n",
      "Batch: 12/27\n",
      "Batch: 13/27\n",
      "Batch: 14/27\n",
      "Batch: 15/27\n",
      "Batch: 16/27\n",
      "Batch: 17/27\n",
      "Batch: 18/27\n",
      "Batch: 19/27\n",
      "Batch: 20/27\n",
      "Batch: 21/27\n",
      "Batch: 22/27\n",
      "Batch: 23/27\n",
      "Batch: 24/27\n",
      "Batch: 25/27\n",
      "Batch: 26/27\n",
      "Batch: 27/27\n",
      "Epoch: 5/15\n",
      "Batch: 1/27\n",
      "Batch: 2/27\n",
      "Batch: 3/27\n",
      "Batch: 4/27\n",
      "Batch: 5/27\n",
      "Batch: 6/27\n",
      "Batch: 7/27\n",
      "Batch: 8/27\n",
      "Batch: 9/27\n",
      "Batch: 10/27\n",
      "Batch: 11/27\n",
      "Batch: 12/27\n",
      "Batch: 13/27\n",
      "Batch: 14/27\n",
      "Batch: 15/27\n",
      "Batch: 16/27\n",
      "Batch: 17/27\n",
      "Batch: 18/27\n",
      "Batch: 19/27\n",
      "Batch: 20/27\n",
      "Batch: 21/27\n",
      "Batch: 22/27\n",
      "Batch: 23/27\n",
      "Batch: 24/27\n",
      "Batch: 25/27\n",
      "Batch: 26/27\n",
      "Batch: 27/27\n",
      "Epoch: 6/15\n",
      "Batch: 1/27\n",
      "Batch: 2/27\n",
      "Batch: 3/27\n",
      "Batch: 4/27\n",
      "Batch: 5/27\n",
      "Batch: 6/27\n",
      "Batch: 7/27\n",
      "Batch: 8/27\n",
      "Batch: 9/27\n",
      "Batch: 10/27\n",
      "Batch: 11/27\n",
      "Batch: 12/27\n",
      "Batch: 13/27\n",
      "Batch: 14/27\n",
      "Batch: 15/27\n",
      "Batch: 16/27\n",
      "Batch: 17/27\n",
      "Batch: 18/27\n",
      "Batch: 19/27\n",
      "Batch: 20/27\n",
      "Batch: 21/27\n",
      "Batch: 22/27\n",
      "Batch: 23/27\n",
      "Batch: 24/27\n",
      "Batch: 25/27\n",
      "Batch: 26/27\n",
      "Batch: 27/27\n",
      "Epoch: 7/15\n",
      "Batch: 1/27\n",
      "Batch: 2/27\n",
      "Batch: 3/27\n",
      "Batch: 4/27\n",
      "Batch: 5/27\n",
      "Batch: 6/27\n",
      "Batch: 7/27\n",
      "Batch: 8/27\n",
      "Batch: 9/27\n",
      "Batch: 10/27\n",
      "Batch: 11/27\n",
      "Batch: 12/27\n",
      "Batch: 13/27\n",
      "Batch: 14/27\n",
      "Batch: 15/27\n",
      "Batch: 16/27\n",
      "Batch: 17/27\n",
      "Batch: 18/27\n",
      "Batch: 19/27\n",
      "Batch: 20/27\n",
      "Batch: 21/27\n",
      "Batch: 22/27\n",
      "Batch: 23/27\n",
      "Batch: 24/27\n",
      "Batch: 25/27\n",
      "Batch: 26/27\n",
      "Batch: 27/27\n",
      "Epoch: 8/15\n",
      "Batch: 1/27\n",
      "Batch: 2/27\n",
      "Batch: 3/27\n",
      "Batch: 4/27\n",
      "Batch: 5/27\n",
      "Batch: 6/27\n",
      "Batch: 7/27\n",
      "Batch: 8/27\n",
      "Batch: 9/27\n",
      "Batch: 10/27\n",
      "Batch: 11/27\n",
      "Batch: 12/27\n",
      "Batch: 13/27\n",
      "Batch: 14/27\n",
      "Batch: 15/27\n",
      "Batch: 16/27\n",
      "Batch: 17/27\n",
      "Batch: 18/27\n",
      "Batch: 19/27\n",
      "Batch: 20/27\n",
      "Batch: 21/27\n",
      "Batch: 22/27\n",
      "Batch: 23/27\n",
      "Batch: 24/27\n",
      "Batch: 25/27\n",
      "Batch: 26/27\n",
      "Batch: 27/27\n",
      "Epoch: 9/15\n",
      "Batch: 1/27\n",
      "Batch: 2/27\n",
      "Batch: 3/27\n",
      "Batch: 4/27\n",
      "Batch: 5/27\n",
      "Batch: 6/27\n",
      "Batch: 7/27\n",
      "Batch: 8/27\n",
      "Batch: 9/27\n",
      "Batch: 10/27\n",
      "Batch: 11/27\n",
      "Batch: 12/27\n",
      "Batch: 13/27\n",
      "Batch: 14/27\n",
      "Batch: 15/27\n",
      "Batch: 16/27\n",
      "Batch: 17/27\n",
      "Batch: 18/27\n",
      "Batch: 19/27\n",
      "Batch: 20/27\n",
      "Batch: 21/27\n",
      "Batch: 22/27\n",
      "Batch: 23/27\n",
      "Batch: 24/27\n",
      "Batch: 25/27\n",
      "Batch: 26/27\n",
      "Batch: 27/27\n",
      "Epoch: 10/15\n",
      "Batch: 1/27\n",
      "Batch: 2/27\n",
      "Batch: 3/27\n",
      "Batch: 4/27\n",
      "Batch: 5/27\n",
      "Batch: 6/27\n",
      "Batch: 7/27\n",
      "Batch: 8/27\n",
      "Batch: 9/27\n",
      "Batch: 10/27\n",
      "Batch: 11/27\n",
      "Batch: 12/27\n",
      "Batch: 13/27\n",
      "Batch: 14/27\n",
      "Batch: 15/27\n",
      "Batch: 16/27\n",
      "Batch: 17/27\n",
      "Batch: 18/27\n",
      "Batch: 19/27\n",
      "Batch: 20/27\n",
      "Batch: 21/27\n",
      "Batch: 22/27\n",
      "Batch: 23/27\n",
      "Batch: 24/27\n",
      "Batch: 25/27\n",
      "Batch: 26/27\n",
      "Batch: 27/27\n",
      "Epoch: 11/15\n",
      "Batch: 1/27\n",
      "Batch: 2/27\n",
      "Batch: 3/27\n",
      "Batch: 4/27\n",
      "Batch: 5/27\n",
      "Batch: 6/27\n",
      "Batch: 7/27\n",
      "Batch: 8/27\n",
      "Batch: 9/27\n",
      "Batch: 10/27\n",
      "Batch: 11/27\n",
      "Batch: 12/27\n",
      "Batch: 13/27\n",
      "Batch: 14/27\n",
      "Batch: 15/27\n",
      "Batch: 16/27\n",
      "Batch: 17/27\n",
      "Batch: 18/27\n",
      "Batch: 19/27\n",
      "Batch: 20/27\n",
      "Batch: 21/27\n",
      "Batch: 22/27\n",
      "Batch: 23/27\n",
      "Batch: 24/27\n",
      "Batch: 25/27\n",
      "Batch: 26/27\n",
      "Batch: 27/27\n",
      "Epoch: 12/15\n",
      "Batch: 1/27\n",
      "Batch: 2/27\n",
      "Batch: 3/27\n",
      "Batch: 4/27\n",
      "Batch: 5/27\n",
      "Batch: 6/27\n",
      "Batch: 7/27\n",
      "Batch: 8/27\n",
      "Batch: 9/27\n",
      "Batch: 10/27\n",
      "Batch: 11/27\n",
      "Batch: 12/27\n",
      "Batch: 13/27\n",
      "Batch: 14/27\n",
      "Batch: 15/27\n",
      "Batch: 16/27\n",
      "Batch: 17/27\n",
      "Batch: 18/27\n",
      "Batch: 19/27\n",
      "Batch: 20/27\n",
      "Batch: 21/27\n",
      "Batch: 22/27\n",
      "Batch: 23/27\n",
      "Batch: 24/27\n",
      "Batch: 25/27\n",
      "Batch: 26/27\n",
      "Batch: 27/27\n",
      "Epoch: 13/15\n",
      "Batch: 1/27\n",
      "Batch: 2/27\n",
      "Batch: 3/27\n",
      "Batch: 4/27\n",
      "Batch: 5/27\n",
      "Batch: 6/27\n",
      "Batch: 7/27\n",
      "Batch: 8/27\n",
      "Batch: 9/27\n",
      "Batch: 10/27\n",
      "Batch: 11/27\n",
      "Batch: 12/27\n",
      "Batch: 13/27\n",
      "Batch: 14/27\n",
      "Batch: 15/27\n",
      "Batch: 16/27\n",
      "Batch: 17/27\n",
      "Batch: 18/27\n",
      "Batch: 19/27\n",
      "Batch: 20/27\n",
      "Batch: 21/27\n",
      "Batch: 22/27\n",
      "Batch: 23/27\n",
      "Batch: 24/27\n",
      "Batch: 25/27\n",
      "Batch: 26/27\n",
      "Batch: 27/27\n",
      "Epoch: 14/15\n",
      "Batch: 1/27\n",
      "Batch: 2/27\n",
      "Batch: 3/27\n",
      "Batch: 4/27\n",
      "Batch: 5/27\n",
      "Batch: 6/27\n",
      "Batch: 7/27\n",
      "Batch: 8/27\n",
      "Batch: 9/27\n",
      "Batch: 10/27\n",
      "Batch: 11/27\n",
      "Batch: 12/27\n",
      "Batch: 13/27\n",
      "Batch: 14/27\n",
      "Batch: 15/27\n",
      "Batch: 16/27\n",
      "Batch: 17/27\n",
      "Batch: 18/27\n",
      "Batch: 19/27\n",
      "Batch: 20/27\n",
      "Batch: 21/27\n",
      "Batch: 22/27\n",
      "Batch: 23/27\n",
      "Batch: 24/27\n",
      "Batch: 25/27\n",
      "Batch: 26/27\n",
      "Batch: 27/27\n",
      "Epoch: 15/15\n",
      "Batch: 1/27\n",
      "Batch: 2/27\n",
      "Batch: 3/27\n",
      "Batch: 4/27\n",
      "Batch: 5/27\n",
      "Batch: 6/27\n",
      "Batch: 7/27\n",
      "Batch: 8/27\n",
      "Batch: 9/27\n",
      "Batch: 10/27\n",
      "Batch: 11/27\n",
      "Batch: 12/27\n",
      "Batch: 13/27\n",
      "Batch: 14/27\n",
      "Batch: 15/27\n",
      "Batch: 16/27\n",
      "Batch: 17/27\n",
      "Batch: 18/27\n",
      "Batch: 19/27\n",
      "Batch: 20/27\n",
      "Batch: 21/27\n",
      "Batch: 22/27\n",
      "Batch: 23/27\n",
      "Batch: 24/27\n",
      "Batch: 25/27\n",
      "Batch: 26/27\n",
      "Batch: 27/27\n",
      "Dimension: Probability\n",
      "Epoch: 1/15\n",
      "Batch: 1/27\n",
      "Batch: 2/27\n",
      "Batch: 3/27\n",
      "Batch: 4/27\n",
      "Batch: 5/27\n",
      "Batch: 6/27\n",
      "Batch: 7/27\n",
      "Batch: 8/27\n",
      "Batch: 9/27\n",
      "Batch: 10/27\n",
      "Batch: 11/27\n",
      "Batch: 12/27\n",
      "Batch: 13/27\n",
      "Batch: 14/27\n",
      "Batch: 15/27\n",
      "Batch: 16/27\n",
      "Batch: 17/27\n",
      "Batch: 18/27\n",
      "Batch: 19/27\n",
      "Batch: 20/27\n",
      "Batch: 21/27\n",
      "Batch: 22/27\n",
      "Batch: 23/27\n",
      "Batch: 24/27\n",
      "Batch: 25/27\n",
      "Batch: 26/27\n",
      "Batch: 27/27\n",
      "Epoch: 2/15\n",
      "Batch: 1/27\n",
      "Batch: 2/27\n",
      "Batch: 3/27\n",
      "Batch: 4/27\n",
      "Batch: 5/27\n",
      "Batch: 6/27\n",
      "Batch: 7/27\n",
      "Batch: 8/27\n",
      "Batch: 9/27\n",
      "Batch: 10/27\n",
      "Batch: 11/27\n",
      "Batch: 12/27\n",
      "Batch: 13/27\n",
      "Batch: 14/27\n",
      "Batch: 15/27\n",
      "Batch: 16/27\n",
      "Batch: 17/27\n",
      "Batch: 18/27\n",
      "Batch: 19/27\n",
      "Batch: 20/27\n",
      "Batch: 21/27\n",
      "Batch: 22/27\n",
      "Batch: 23/27\n",
      "Batch: 24/27\n",
      "Batch: 25/27\n",
      "Batch: 26/27\n",
      "Batch: 27/27\n",
      "Epoch: 3/15\n",
      "Batch: 1/27\n",
      "Batch: 2/27\n",
      "Batch: 3/27\n",
      "Batch: 4/27\n",
      "Batch: 5/27\n",
      "Batch: 6/27\n",
      "Batch: 7/27\n",
      "Batch: 8/27\n",
      "Batch: 9/27\n",
      "Batch: 10/27\n",
      "Batch: 11/27\n",
      "Batch: 12/27\n",
      "Batch: 13/27\n",
      "Batch: 14/27\n",
      "Batch: 15/27\n",
      "Batch: 16/27\n",
      "Batch: 17/27\n",
      "Batch: 18/27\n",
      "Batch: 19/27\n",
      "Batch: 20/27\n",
      "Batch: 21/27\n",
      "Batch: 22/27\n",
      "Batch: 23/27\n",
      "Batch: 24/27\n",
      "Batch: 25/27\n",
      "Batch: 26/27\n",
      "Batch: 27/27\n",
      "Epoch: 4/15\n",
      "Batch: 1/27\n",
      "Batch: 2/27\n",
      "Batch: 3/27\n",
      "Batch: 4/27\n",
      "Batch: 5/27\n",
      "Batch: 6/27\n",
      "Batch: 7/27\n",
      "Batch: 8/27\n",
      "Batch: 9/27\n",
      "Batch: 10/27\n",
      "Batch: 11/27\n",
      "Batch: 12/27\n",
      "Batch: 13/27\n",
      "Batch: 14/27\n",
      "Batch: 15/27\n",
      "Batch: 16/27\n",
      "Batch: 17/27\n",
      "Batch: 18/27\n",
      "Batch: 19/27\n",
      "Batch: 20/27\n",
      "Batch: 21/27\n",
      "Batch: 22/27\n",
      "Batch: 23/27\n",
      "Batch: 24/27\n",
      "Batch: 25/27\n",
      "Batch: 26/27\n",
      "Batch: 27/27\n",
      "Epoch: 5/15\n",
      "Batch: 1/27\n",
      "Batch: 2/27\n",
      "Batch: 3/27\n",
      "Batch: 4/27\n",
      "Batch: 5/27\n",
      "Batch: 6/27\n",
      "Batch: 7/27\n",
      "Batch: 8/27\n",
      "Batch: 9/27\n",
      "Batch: 10/27\n",
      "Batch: 11/27\n",
      "Batch: 12/27\n",
      "Batch: 13/27\n",
      "Batch: 14/27\n",
      "Batch: 15/27\n",
      "Batch: 16/27\n",
      "Batch: 17/27\n",
      "Batch: 18/27\n",
      "Batch: 19/27\n",
      "Batch: 20/27\n",
      "Batch: 21/27\n",
      "Batch: 22/27\n",
      "Batch: 23/27\n",
      "Batch: 24/27\n",
      "Batch: 25/27\n",
      "Batch: 26/27\n",
      "Batch: 27/27\n",
      "Epoch: 6/15\n",
      "Batch: 1/27\n",
      "Batch: 2/27\n",
      "Batch: 3/27\n",
      "Batch: 4/27\n",
      "Batch: 5/27\n",
      "Batch: 6/27\n",
      "Batch: 7/27\n",
      "Batch: 8/27\n",
      "Batch: 9/27\n",
      "Batch: 10/27\n",
      "Batch: 11/27\n",
      "Batch: 12/27\n",
      "Batch: 13/27\n",
      "Batch: 14/27\n",
      "Batch: 15/27\n",
      "Batch: 16/27\n",
      "Batch: 17/27\n",
      "Batch: 18/27\n",
      "Batch: 19/27\n",
      "Batch: 20/27\n",
      "Batch: 21/27\n",
      "Batch: 22/27\n",
      "Batch: 23/27\n",
      "Batch: 24/27\n",
      "Batch: 25/27\n",
      "Batch: 26/27\n",
      "Batch: 27/27\n",
      "Epoch: 7/15\n",
      "Batch: 1/27\n",
      "Batch: 2/27\n",
      "Batch: 3/27\n",
      "Batch: 4/27\n",
      "Batch: 5/27\n",
      "Batch: 6/27\n",
      "Batch: 7/27\n",
      "Batch: 8/27\n",
      "Batch: 9/27\n",
      "Batch: 10/27\n",
      "Batch: 11/27\n",
      "Batch: 12/27\n",
      "Batch: 13/27\n",
      "Batch: 14/27\n",
      "Batch: 15/27\n",
      "Batch: 16/27\n",
      "Batch: 17/27\n",
      "Batch: 18/27\n",
      "Batch: 19/27\n",
      "Batch: 20/27\n",
      "Batch: 21/27\n",
      "Batch: 22/27\n",
      "Batch: 23/27\n",
      "Batch: 24/27\n",
      "Batch: 25/27\n",
      "Batch: 26/27\n",
      "Batch: 27/27\n",
      "Epoch: 8/15\n",
      "Batch: 1/27\n",
      "Batch: 2/27\n",
      "Batch: 3/27\n",
      "Batch: 4/27\n",
      "Batch: 5/27\n",
      "Batch: 6/27\n",
      "Batch: 7/27\n",
      "Batch: 8/27\n",
      "Batch: 9/27\n",
      "Batch: 10/27\n",
      "Batch: 11/27\n",
      "Batch: 12/27\n",
      "Batch: 13/27\n",
      "Batch: 14/27\n",
      "Batch: 15/27\n",
      "Batch: 16/27\n",
      "Batch: 17/27\n",
      "Batch: 18/27\n",
      "Batch: 19/27\n",
      "Batch: 20/27\n",
      "Batch: 21/27\n",
      "Batch: 22/27\n",
      "Batch: 23/27\n",
      "Batch: 24/27\n",
      "Batch: 25/27\n",
      "Batch: 26/27\n",
      "Batch: 27/27\n",
      "Epoch: 9/15\n",
      "Batch: 1/27\n",
      "Batch: 2/27\n",
      "Batch: 3/27\n",
      "Batch: 4/27\n",
      "Batch: 5/27\n",
      "Batch: 6/27\n",
      "Batch: 7/27\n",
      "Batch: 8/27\n",
      "Batch: 9/27\n",
      "Batch: 10/27\n",
      "Batch: 11/27\n",
      "Batch: 12/27\n",
      "Batch: 13/27\n",
      "Batch: 14/27\n",
      "Batch: 15/27\n",
      "Batch: 16/27\n",
      "Batch: 17/27\n",
      "Batch: 18/27\n",
      "Batch: 19/27\n",
      "Batch: 20/27\n",
      "Batch: 21/27\n",
      "Batch: 22/27\n",
      "Batch: 23/27\n",
      "Batch: 24/27\n",
      "Batch: 25/27\n",
      "Batch: 26/27\n",
      "Batch: 27/27\n",
      "Epoch: 10/15\n",
      "Batch: 1/27\n",
      "Batch: 2/27\n",
      "Batch: 3/27\n",
      "Batch: 4/27\n",
      "Batch: 5/27\n",
      "Batch: 6/27\n",
      "Batch: 7/27\n",
      "Batch: 8/27\n",
      "Batch: 9/27\n",
      "Batch: 10/27\n",
      "Batch: 11/27\n",
      "Batch: 12/27\n",
      "Batch: 13/27\n",
      "Batch: 14/27\n",
      "Batch: 15/27\n",
      "Batch: 16/27\n",
      "Batch: 17/27\n",
      "Batch: 18/27\n",
      "Batch: 19/27\n",
      "Batch: 20/27\n",
      "Batch: 21/27\n",
      "Batch: 22/27\n",
      "Batch: 23/27\n",
      "Batch: 24/27\n",
      "Batch: 25/27\n",
      "Batch: 26/27\n",
      "Batch: 27/27\n",
      "Epoch: 11/15\n",
      "Batch: 1/27\n",
      "Batch: 2/27\n",
      "Batch: 3/27\n",
      "Batch: 4/27\n",
      "Batch: 5/27\n",
      "Batch: 6/27\n",
      "Batch: 7/27\n",
      "Batch: 8/27\n",
      "Batch: 9/27\n",
      "Batch: 10/27\n",
      "Batch: 11/27\n",
      "Batch: 12/27\n",
      "Batch: 13/27\n",
      "Batch: 14/27\n",
      "Batch: 15/27\n",
      "Batch: 16/27\n",
      "Batch: 17/27\n",
      "Batch: 18/27\n",
      "Batch: 19/27\n",
      "Batch: 20/27\n",
      "Batch: 21/27\n",
      "Batch: 22/27\n",
      "Batch: 23/27\n",
      "Batch: 24/27\n",
      "Batch: 25/27\n",
      "Batch: 26/27\n",
      "Batch: 27/27\n",
      "Epoch: 12/15\n",
      "Batch: 1/27\n",
      "Batch: 2/27\n",
      "Batch: 3/27\n",
      "Batch: 4/27\n",
      "Batch: 5/27\n",
      "Batch: 6/27\n",
      "Batch: 7/27\n",
      "Batch: 8/27\n",
      "Batch: 9/27\n",
      "Batch: 10/27\n",
      "Batch: 11/27\n",
      "Batch: 12/27\n",
      "Batch: 13/27\n",
      "Batch: 14/27\n",
      "Batch: 15/27\n",
      "Batch: 16/27\n",
      "Batch: 17/27\n",
      "Batch: 18/27\n",
      "Batch: 19/27\n",
      "Batch: 20/27\n",
      "Batch: 21/27\n",
      "Batch: 22/27\n",
      "Batch: 23/27\n",
      "Batch: 24/27\n",
      "Batch: 25/27\n",
      "Batch: 26/27\n",
      "Batch: 27/27\n",
      "Epoch: 13/15\n",
      "Batch: 1/27\n",
      "Batch: 2/27\n",
      "Batch: 3/27\n",
      "Batch: 4/27\n",
      "Batch: 5/27\n",
      "Batch: 6/27\n",
      "Batch: 7/27\n",
      "Batch: 8/27\n",
      "Batch: 9/27\n",
      "Batch: 10/27\n",
      "Batch: 11/27\n",
      "Batch: 12/27\n",
      "Batch: 13/27\n",
      "Batch: 14/27\n",
      "Batch: 15/27\n",
      "Batch: 16/27\n",
      "Batch: 17/27\n",
      "Batch: 18/27\n",
      "Batch: 19/27\n",
      "Batch: 20/27\n",
      "Batch: 21/27\n",
      "Batch: 22/27\n",
      "Batch: 23/27\n",
      "Batch: 24/27\n",
      "Batch: 25/27\n",
      "Batch: 26/27\n",
      "Batch: 27/27\n",
      "Epoch: 14/15\n",
      "Batch: 1/27\n",
      "Batch: 2/27\n",
      "Batch: 3/27\n",
      "Batch: 4/27\n",
      "Batch: 5/27\n",
      "Batch: 6/27\n",
      "Batch: 7/27\n",
      "Batch: 8/27\n",
      "Batch: 9/27\n",
      "Batch: 10/27\n",
      "Batch: 11/27\n",
      "Batch: 12/27\n",
      "Batch: 13/27\n",
      "Batch: 14/27\n",
      "Batch: 15/27\n",
      "Batch: 16/27\n",
      "Batch: 17/27\n",
      "Batch: 18/27\n",
      "Batch: 19/27\n",
      "Batch: 20/27\n",
      "Batch: 21/27\n",
      "Batch: 22/27\n",
      "Batch: 23/27\n",
      "Batch: 24/27\n",
      "Batch: 25/27\n",
      "Batch: 26/27\n",
      "Batch: 27/27\n",
      "Epoch: 15/15\n",
      "Batch: 1/27\n",
      "Batch: 2/27\n",
      "Batch: 3/27\n",
      "Batch: 4/27\n",
      "Batch: 5/27\n",
      "Batch: 6/27\n",
      "Batch: 7/27\n",
      "Batch: 8/27\n",
      "Batch: 9/27\n",
      "Batch: 10/27\n",
      "Batch: 11/27\n",
      "Batch: 12/27\n",
      "Batch: 13/27\n",
      "Batch: 14/27\n",
      "Batch: 15/27\n",
      "Batch: 16/27\n",
      "Batch: 17/27\n",
      "Batch: 18/27\n",
      "Batch: 19/27\n",
      "Batch: 20/27\n",
      "Batch: 21/27\n",
      "Batch: 22/27\n",
      "Batch: 23/27\n",
      "Batch: 24/27\n",
      "Batch: 25/27\n",
      "Batch: 26/27\n",
      "Batch: 27/27\n",
      "Dimension: Condition\n",
      "Epoch: 1/15\n",
      "Batch: 1/27\n",
      "Batch: 2/27\n",
      "Batch: 3/27\n",
      "Batch: 4/27\n",
      "Batch: 5/27\n",
      "Batch: 6/27\n",
      "Batch: 7/27\n",
      "Batch: 8/27\n",
      "Batch: 9/27\n",
      "Batch: 10/27\n",
      "Batch: 11/27\n",
      "Batch: 12/27\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 15\n",
    "batch_size = 64\n",
    "num_batches = len(real_text_token) // batch_size\n",
    "\n",
    "all_disc_loss = [[[None for x in range(num_batches)] for y in range(num_epochs)] for z in range(num_label_dims)]\n",
    "all_gen_loss = [[[None for x in range(num_batches)] for y in range(num_epochs)] for z in range(num_label_dims)]\n",
    "\n",
    "key_idx = 0\n",
    "for key in dims.keys():\n",
    "    print(f\"Dimension: {key}\")\n",
    "    labels = dims[key]\n",
    "\n",
    "    generator = Generator(vocab_size, embedding_dim, latent_dim, label_dim)\n",
    "    discriminator = Discriminator(vocab_size, embedding_dim, label_dim)\n",
    "    generator_optimizer = tf.keras.optimizers.Adam(learning_rate=0.0002)\n",
    "    discriminator_optimizer = tf.keras.optimizers.Adam(learning_rate=0.0002)\n",
    "    generator.compile(loss=generator_loss, optimizer=generator_optimizer)\n",
    "    discriminator.compile(loss=discriminator_loss, optimizer=discriminator_optimizer)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Epoch: {epoch+1}/{num_epochs}\")\n",
    "    \n",
    "        for i in range(num_batches):\n",
    "            print(f\"Batch: {i+1}/{num_batches}\")\n",
    "            start = i * batch_size\n",
    "            end = (i + 1) * batch_size\n",
    "            \n",
    "            real_data = tf.convert_to_tensor(real_text_token[start:end])\n",
    "            real_data_labels = tf.convert_to_tensor(labels[start:end])\n",
    "            noise = tf.random.normal(shape=(batch_size, latent_dim))\n",
    "        \n",
    "            disc_loss, gen_loss = train_step(real_data, real_data_labels, noise)\n",
    "            all_disc_loss[key_idx][epoch][i] = disc_loss\n",
    "            all_gen_loss[key_idx][epoch][i] = gen_loss\n",
    "\n",
    "    generator.save_weights(f\"generator_{key}.weights.h5\")\n",
    "    discriminator.save_weights(f\"discriminator_{key}.weights.h5\")\n",
    "    key_idx += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
